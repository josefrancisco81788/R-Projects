---
title: 'Predictive Analytics in-class exercise on Corporate Bankruptcy Detection'
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("C:\\Users\\User\\Documents\\NYU STERN MSBA\\Module 2\\Data Mining with R\\Case - Explainable AI -- LIME on Neural Network Model for Bankruptcy Prediction")
path <- rstudioapi::getActiveDocumentContext()$path
Encoding(path) <- "UTF-8"
setwd(dirname(path))
```


```{r message=FALSE,  warning=FALSE}
# load the required libraries
library("readxl") # used to read excel files
library("dplyr") # used for data munging 
library("FNN") # used for knn regression (knn.reg function)
library("caret") # used for various predictive models
library("class") # for using confusion matrix function
library("rpart.plot") # used to plot decision tree
library("rpart")  # used for Regression tree
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("PRROC") # top plot ROC curve
library("ROCR") # top plot lift curve
library("tidyverse")
library("skimr")
library(lime)
```


# 1. Classification


## 1.1 Data loading and transformation



```{r }
# Load the Corporate Rations Dataset to predict bankruptcy

corpRatios <- read_excel("CL-bankruptcy.xls", 
     sheet = "data", col_types = c("skip", 
        "text", "text", "numeric", "numeric", 
         "numeric", "numeric", "numeric", 
         "numeric", "numeric", "numeric", 
          "numeric", "numeric", "numeric", 
         "numeric", "numeric", "numeric", 
         "numeric", "numeric", "numeric", 
         "numeric", "numeric", "numeric", 
         "numeric", "numeric", "numeric",
        "numeric"))
     
     
skim(corpRatios)


# create Y and X data frames
corpRatios_y = corpRatios %>% pull("D") %>% as.factor()
# exclude X1 since its a row number
corpRatios_x = corpRatios %>% select(-c("D"))
corpRatios_x$YR <- as.factor(corpRatios_x$YR)
```


Create Training and Testing data sets

```{r }
# 75% of the data is used for training and rest for testing
smp_size <- floor(0.75 * nrow(corpRatios_x))

# randomly select row numbers for training data set
train_ind <- sample(seq_len(nrow(corpRatios_x)), size = smp_size)

# creating test and training sets for x
corpRatios_x_train <- corpRatios_x[train_ind, ]
corpRatios_x_test <- corpRatios_x[-train_ind, ]

# creating test and training sets for y
corpRatios_y_train <- corpRatios_y[train_ind]
corpRatios_y_test <- corpRatios_y[-train_ind]

# Create an empty data frame to store results from different models
clf_results <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results) <- c("Model", "Accuracy", "Precision", "Recall", "F1")

# Create an empty data frame to store TP, TN, FP and FN values
cost_benefit_df <- data.frame(matrix(ncol = 5, nrow = 0))
names(cost_benefit_df) <- c("Model", "TP", "FN", "FP", "TN")


```

**Cross validation**

It is a technique to use same training data but some portion of it for training and rest for validation of model. This technique reduces chances of overfitting

**Hyperparamter tuning**

We provide a list of hyperparameters to train the model. This helps in identifying best set of hyperparameters for a given model like Decision tree. **train** function in caret library automatically stores the information of the best model and its hyperparameters.



## 1.3 Decision Tree Classification 

```{r }
# Convert corpRatios_x_train to a data frame
corpRatios_x_train_df <- as.data.frame(corpRatios_x_train)

# Cross validation
cross_validation <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10,
                                ## repeated three times
                                repeats = 3)
# Hyperparameter tuning
# maxdepth =  the maximum depth of the tree that will be created or
# the length of the longest path from the tree root to a leaf.

Param_Grid <-  data.frame(maxdepth = 2:10)

dtree_fit <- train(corpRatios_x_train_df,
                   corpRatios_y_train, 
                   method = "rpart2",
                   # split - criteria to split nodes
                   parms = list(split = "gini"),
                  tuneGrid = Param_Grid,
                   trControl = cross_validation,
                  # preProc -  perform listed pre-processing to predictor dataframe
                   preProc = c("center", "scale"))

# check the accuracy for different models
dtree_fit
```

```{r }
# print the final model
dtree_fit$finalModel
```

```{r }
# Plot decision tree
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```

```{r }
# Predict on test data
dtree_predict <- predict(dtree_fit, newdata = corpRatios_x_test)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(dtree_predict,  corpRatios_y_test , positive = "1")

# Add results into clf_results dataframe
x2 <- confusionMatrix(dtree_predict,  corpRatios_y_test , positive = "1")[["overall"]]
y2 <- confusionMatrix(dtree_predict,  corpRatios_y_test , positive = "1")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Decision Tree", 
                                             Accuracy = round (x2[["Accuracy"]],3), 
                                            Precision = round (y2[["Precision"]],3), 
                                            Recall = round (y2[["Recall"]],3), 
                                            F1 = round (y2[["F1"]],3))

# Print Accuracy and F1 score

cat("Accuarcy is ", round(x2[["Accuracy"]],3), "and F1 is ", round (y2[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a2 <- confusionMatrix(dtree_predict,  corpRatios_y_test )

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Decision Tree", 
                                             TP = a2[["table"]][4], 
                                             FN = a2[["table"]][3], 
                                             FP = a2[["table"]][2], 
                                             TN = a2[["table"]][1])

```
